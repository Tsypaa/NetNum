{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.where(self.input > 0, self.input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        relu_grad = self.input > 0\n",
    "        return dout*relu_grad\n",
    "        # return np.where(dout > 0, 1, 0)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "class CE:\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.shape[0]\n",
    "        max_values = np.max(x, axis=1, keepdims=True)\n",
    "        x_shifted = x - max_values\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        self.y_pred = exp_x / np.sum(exp_x, axis=1, keepdims=True)  # softmax\n",
    "        self.y_true = y  # one-hot вектор\n",
    "        return -np.sum(self.y_true * np.log(self.y_pred + 1e-9)) / batch_size  # 1e-9 для численной стабильности\n",
    "    \n",
    "    def backward(self, out, y):\n",
    "        batch_size = out.shape[0]\n",
    "        return (self.y_pred - y) / batch_size  # softmax(x) - y\n",
    "\n",
    "    def __call__(self, out, y):\n",
    "        return self.forward(out, y)\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0, keepdims=True)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.uniform(-1 / np.sqrt(input_size), 1 / np.sqrt(input_size), (input_size, output_size))\n",
    "        self.bias = np.random.randn(output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, error, lr):\n",
    "        d_output = np.dot(error, self.weights.T)\n",
    "        self.weights -= lr * np.dot(self.input.T, error)\n",
    "        self.bias -= lr * np.sum(error, axis=0)\n",
    "        return d_output\n",
    "    \n",
    "    # def weights_save(self, i):\n",
    "    #     np.save(f'best_weights_layer{i}.npy', self.weights)\n",
    "    #     np.save(f'best_bias_layer{i}.npy', self.bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetNum():\n",
    "    def __init__(self, input_size, input_size_2, input_size_3, output_size, lr):\n",
    "        self.lr = lr\n",
    "        self.layer_1 = Layer(input_size, input_size_2)\n",
    "        self.layer_1_act = ReLU()\n",
    "        self.layer_2 = Layer(input_size_2, input_size_3)\n",
    "        self.layer_2_act = ReLU()\n",
    "        self.layer_3 = Layer(input_size_3, output_size)\n",
    "        self.ce = CE()\n",
    "\n",
    "    def forward_prop(self, x):\n",
    "        x = self.layer_1.forward(x)\n",
    "        layer_1_acc = self.layer_1_act(x)\n",
    "        x = self.layer_2.forward(layer_1_acc)\n",
    "        layer_2_acc = self.layer_2_act(x)\n",
    "        x = self.layer_3.forward(layer_2_acc)\n",
    "        return x\n",
    "\n",
    "    def backward_prop(self, out, y):\n",
    "        # Не самое лучшее решение, но я не хочу заморачиваться с градиентами\n",
    "        # Но на будущее - лучше CE вынести в метод обучения\n",
    "        dout = self.ce.backward(out, y)\n",
    "        x = self.layer_3.backward(dout, self.lr)\n",
    "        x = self.layer_2_act.backward(x)\n",
    "        x = self.layer_2.backward(x, self.lr)\n",
    "        x = self.layer_1_act.backward(x)\n",
    "        x = self.layer_1.backward(x, self.lr)\n",
    "\n",
    "    def train(self, X, y, epochs, batch_size=32, split=0.8, shuffle=True, seed=42):\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "        \n",
    "        # Разделяем данные на обучающую и валидационную выборки\n",
    "        split_idx = int(len(X) * split)\n",
    "        X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        pbar = tqdm(range(epochs))\n",
    "\n",
    "        # Рандомизируем данные\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            epoch_loss = 0\n",
    "            # Перемешиваем данные\n",
    "            if shuffle:\n",
    "                indices = np.random.permutation(len(X_train))\n",
    "                X_shuffled = X_train[indices]\n",
    "                y_shuffled = y_train[indices]\n",
    "            \n",
    "            # Разбиваем на батчи\n",
    "            num_batches = 0\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                # Прямое распространение\n",
    "                out = self.forward_prop(X_batch)\n",
    "                \n",
    "                # Вычисляем функцию потерь\n",
    "                batch_loss = self.ce(out, y_batch)\n",
    "                epoch_loss += batch_loss\n",
    "                \n",
    "                # Обратное распространение\n",
    "                self.backward_prop(out, y_batch)\n",
    "                num_batches += 1\n",
    "            \n",
    "            # Средняя потеря за эпоху\n",
    "            avg_epoch_loss = epoch_loss / num_batches\n",
    "            train_loss_history.append(avg_epoch_loss)\n",
    "            \n",
    "            # Проверяем на валидационной выборке\n",
    "            val_loss = 0\n",
    "            num_val_batches = 0\n",
    "            \n",
    "            for i in range(0, len(X_val), batch_size):\n",
    "                X_val_batch = X_val[i:i+batch_size]\n",
    "                y_val_batch = y_val[i:i+batch_size]\n",
    "                \n",
    "                # Только прямое распространение для валидации\n",
    "                val_out = self.forward_prop(X_val_batch)\n",
    "                val_batch_loss = self.ce(val_out, y_val_batch)\n",
    "                val_loss += val_batch_loss\n",
    "                num_val_batches += 1\n",
    "            \n",
    "            avg_val_loss = val_loss / num_val_batches if num_val_batches > 0 else 0\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "                \n",
    "            # Обновляем описание прогресс-бара с информацией о валидации и тренировке\n",
    "            pbar.set_description(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_epoch_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "                        \n",
    "        return train_loss_history, val_loss_history\n",
    "    \n",
    "    def predict(self, x):\n",
    "        output = self.forward_prop(x)\n",
    "        return output\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        correct = 0\n",
    "        total = len(X)\n",
    "        \n",
    "        for i in range(total):\n",
    "            pred = self.predict(X[i].reshape(1, -1))\n",
    "            pred_class = np.argmax(pred)\n",
    "            true_class = np.argmax(y[i])\n",
    "            if pred_class == true_class:\n",
    "                correct += 1\n",
    "                \n",
    "        accuracy = correct / total\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mnist_train.csv')\n",
    "\n",
    "onehot = pd.get_dummies(df['label']).values\n",
    "\n",
    "X = df.drop('label', axis=1).values / 255.0\n",
    "\n",
    "indices_by_class = [df.index[df['label'] == i] for i in range(10)]\n",
    "valid_indices = np.concatenate(indices_by_class)\n",
    "np.random.shuffle(valid_indices)\n",
    "\n",
    "X_train = X[valid_indices][:-100]\n",
    "y_train = onehot[valid_indices][:-100]\n",
    "\n",
    "X_test = X[valid_indices][-100:]\n",
    "y_test = onehot[valid_indices][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 2.284976 | Val Loss: 2.219389:   2%|▎         | 1/40 [00:04<03:03,  4.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m NN = NetNum(input_size=\u001b[32m784\u001b[39m, input_size_2=\u001b[32m256\u001b[39m, input_size_3=\u001b[32m64\u001b[39m, output_size=\u001b[32m10\u001b[39m, lr=\u001b[32m0.005\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_loss, val_loss = \u001b[43mNN\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m7\u001b[39m))\n\u001b[32m      5\u001b[39m plt.plot(train_loss, \u001b[33m'\u001b[39m\u001b[33m-b\u001b[39m\u001b[33m'\u001b[39m,label=\u001b[33m\"\u001b[39m\u001b[33mTrain Loss\u001b[39m\u001b[33m\"\u001b[39m, lw=\u001b[32m3\u001b[39m, ms=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mNetNum.train\u001b[39m\u001b[34m(self, X, y, epochs, batch_size, split, shuffle, seed)\u001b[39m\n\u001b[32m     65\u001b[39m     epoch_loss += batch_loss\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# Обратное распространение\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     num_batches += \u001b[32m1\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Средняя потеря за эпоху\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mNetNum.backward_prop\u001b[39m\u001b[34m(self, out, y)\u001b[39m\n\u001b[32m     23\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_3.backward(dout, \u001b[38;5;28mself\u001b[39m.lr)\n\u001b[32m     24\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_2_act.backward(x)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_1_act.backward(x)\n\u001b[32m     27\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_1.backward(x, \u001b[38;5;28mself\u001b[39m.lr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mLayer.backward\u001b[39m\u001b[34m(self, error, lr)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, error, lr):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     d_output = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mself\u001b[39m.weights -= lr * np.dot(\u001b[38;5;28mself\u001b[39m.input.T, error)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias -= lr * np.sum(error, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "NN = NetNum(input_size=784, input_size_2=256, input_size_3=64, output_size=10, lr=0.005)\n",
    "train_loss, val_loss = NN.train(X_train, y_train, epochs=40, batch_size=128)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(train_loss, '-b',label=\"Train Loss\", lw=3, ms=0)\n",
    "plt.plot(val_loss, '-.r', label=\"Validation Loss\", lw=3, ms=0)\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.accuracy(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
